{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF file\n",
    "pdf_file_path = 'C:/Users/yrui7/Documents/vscode/Arxiv/AI/ai_1.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read text from pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "* equal contribution  raise  – radiology ai  safe ty, an end-\n",
      "to-end lifecycle approach   \n",
      " \n",
      "m. jorge cardoso  * \n",
      "school of biomedical engineering and imaging sciences, king’s college london, london, uk  \n",
      " \n",
      "julia moosbauer  * \n",
      "deepc gmbh, munich, germany ;  \n",
      "department of statistics, ludwig maximilian university munich, munich, germany  \n",
      " \n",
      "tessa s. cook  \n",
      "department of radiology, perelman school of medicine at the university of pennsylvania, \n",
      "philadelphia, pa, usa  \n",
      " \n",
      "b. selnur erdal,  \n",
      "center  for augmented intelligence in imaging, department of radiology, mayo clinic, jacksonville, \n",
      "florida, usa  \n",
      " \n",
      "brad genereaux  \n",
      "nvidia, santa clara, ca, usa  \n",
      " \n",
      "vikash gupta  \n",
      "center for augmented intelligence in imaging,  department of radiology,  mayo clinic, jacksonville, \n",
      "florida, usa  \n",
      " \n",
      "bennett a. landman  \n",
      "vanderbilt university, nashville, tennessee, usa  \n",
      " \n",
      "tiarna lee  \n",
      "school of biomedical engineering and imaging sciences, king’s college london, london, uk  \n",
      " \n",
      "parashkev nachev  \n",
      "ucl queen square institute of neurology , university college london, london, uk  \n",
      " \n",
      "elanchezhian somasundaram  \n",
      "department of radiology, cincinnati children’s hospital medical center, cincinnati, oh, usa; \n",
      "department of pediatrics, university of cincinnati college of medicine, cincinnati, oh, usa  \n",
      " \n",
      "ronald  m. summers  \n",
      "imaging biomarkers and computer -aided diagnosis laboratory , radiology and imaging sciences , \n",
      "nih clinical center , bethesda , maryland, usa  \n",
      " \n",
      "khaled younis  \n",
      "phillips research north america, cambridge, md, usa  \n",
      " \n",
      "sebastien ourselin  * \n",
      "school of biomedical engineering and imaging sciences, king’s college london, london, uk  \n",
      " \n",
      "franz mj pfister  * \n",
      "deepc  gmbh, munich, german \n",
      " \n",
      " \n",
      " abstract  \n",
      "the integration of ai into radiology introduces opportunities for improved clinical care  provision \n",
      "and efficiency but it  demands a meticulous approach to mitigate potential risks  as with any other new \n",
      "technology . beginning with rigorous pre -deployment evaluation and validation, the focus should be \n",
      "on ensuring models meet the highest standards of safety , effectiveness  and efficacy  for their intended \n",
      "applications. input and output guardrails implemented during production  usage act as an additional \n",
      "layer of protection, identifying and addressing individual failures as they occur. continuous post -\n",
      "deployment monitoring allows for t racking  population -level performance ( data  drift), fairness, and \n",
      "value delivery over time.  scheduling  reviews of post -deployment  model  performance  and educating \n",
      "radiologists about  new algorithmic -driven findings  is critical  for ai to be effective in clinical practice.  \n",
      "recognizing that no single ai solution can provide absolute assurance  even when limited to its \n",
      "intended use , the synergistic application of quality assurance at multiple levels - regulatory, clinical, \n",
      "technical, and ethical - is emphasized. collaborative efforts between stakeholders spanning healthcare  \n",
      "systems, industry, academia, and government are imperative to address the multifaceted challenges \n",
      "involved.  trust in ai is an earned privilege, contingent on  a broad set of goals , among them \n",
      "transparently demonstrating that the ai adheres to the same rigorous safety , effectiven ess and \n",
      "efficacy standards as other established medical technologies. by doing so, developers can instil  \n",
      "confidence among providers and patients alike, enabling the responsible scaling of ai and the \n",
      "realization of its potential benefits. the roadmap presented herein aims to expedite the achievement \n",
      "of deployable, reliable, and safe ai in radiology.  \n",
      "1. introduction  \n",
      "the safe, effective, and ethical deployment of ai systems in radiology requires comprehensive \n",
      "efforts spanning the entire ai product lifecycle. both a  recent us president’s executive order on the \n",
      "safe, secure, and trustworthy development and use of artificial intelligence  [1] and the eu ai act [2] \n",
      "have further highlighted the importance of this topic in high impact areas such as healthcare. \n",
      "healthcare  ai already has strong regulatory  guidance , with entities such as the food and drug \n",
      "administration (fda) and european medicines agency (ema) hav ing stringent guidelines governing \n",
      "clinical ai given its direct impact on patient health and safety  [3]. premarket testing and validation \n",
      "requirements, for example, aim to minimize risks from inaccurate or biased models, while post -market \n",
      "surveillance continuously monitors performance during production deployment . meeting regulatory \n",
      "standards is thus  already  a legal obligation for healthcare ai.  \n",
      "primum non nocere  is one of principal precepts of bioethics and  guides medical practitioners to \n",
      "first do no harm. with the increasing use of ai for patient care, it is imperative that models are bound \n",
      "to the same standards by avoiding preventable errors that could worsen outcomes. the moral \n",
      "mandate to equitably serve all patients , regardless of background, motivates minimising algorithmic \n",
      "errors while contributing towards building appropriate user trust; thus, both deontological and \n",
      "utilitarian ethics necessitate healthcare ai safety , effectiveness  and efficacy . \n",
      "ai models , which form part of software as a medical device (samd) solution, should be  developed \n",
      "with inbuilt explicit (e.g.  bias and fairness ) or implicit  (e.g. explainability ) safety mechanisms. by \n",
      "making downstream users privy to these factors, ai developers aim to mitigate concerns and minimize  \n",
      " \n",
      " \n",
      " risks inherently associated with using these models in a live scenario. many samd solutions, however, \n",
      "do not provide such mechanisms ; they often rely on the confidence in their large development \n",
      "datasets to model the target patient population distribution relatively well, and software environment \n",
      "assumptions. this is due to the complexity and cost of implementing systematic pre -deploym ent \n",
      "quality control and quality assurance procedures, live monitoring systems, and scheduled post -\n",
      "deployment surveillance. w ith nearly 700  ai-based samd solutions [4] on the market , replicating this \n",
      "infrastructure on a per -solutions basis would be very complex due to cost and scalability. a practical  \n",
      "solution to this scalability problem is to implement  these quality, safety, and monitoring mechanisms \n",
      "at the ai orchestration platform level  [5], ensuring  trustworthy and safe ai is procured, commissioned, \n",
      "delivered, and integrated uniformly across all hospital ai solutions [6]. such a platform approach \n",
      "would also centralise the assessment of mid- to long -term outcomes , enabling a better understanding \n",
      "of solution value .  \n",
      " \n",
      "trustworthy and safe ai needs a holistic life -cycle approach, tackling pre -, peri - and post -\n",
      "deployment stages. in the pre -deployment stage, regulatory due diligence ensures models meet \n",
      "guidelines for analytic and clinical validity set by regulatory bodies such as the fda , mhra  or ema, \n",
      "and institutional ai governance councils. extensive quality assurance testing of software allows for the  \n",
      "identification of  deployment time bugs and/or computational performance issues, while independent  \n",
      "validation studies on diverse data can help identify potential model accuracy issues and \n",
      "generalisability limitations prior to deployment. during live deployment, input data quality needs to \n",
      "be closely monitored to ensure it matches the quality and type o f data used during pre -deployment \n",
      "testing, while output guardrails such as uncertainty estimates,  and human -in-the-loop reviews act as \n",
      "a redundant safety net allowing models to  defer uncertain decisions to clinicians. lastly, in the post -\n",
      "deployment phase, continued surveillance ensures models maintain their safety and effectiveness, \n",
      "that algorithm performance drift caused by changes in data or population characteristics are \n",
      "meas ured appropriately and models re -calibrated, and fairness and bias assessments proactively \n",
      "uncover inequities in model behaviour  between patient subgroups.  \n",
      " \n",
      "this end -to-end approach to trustworthy ai deployment, summarized  in figure 1  below, \n",
      "emphasizes  safeguards across the ai lifecycle , including regulatory, clinical, technical, and ethical \n",
      "considerations. when rigorously implemented, it fosters provider confidence in applying ai \n",
      "technologies to improve patient outcomes and quality of care. institutional support and collaboration \n",
      "between  diverse experts are crucial for navigating the sociotechnical complexities of ai in radiology.  \n",
      " \n",
      " \n",
      "figure 1 – diagram  representing the proposed radiology ai safety lifecycle  approach   \n",
      " \n",
      " \n",
      " \n",
      " 2. pre-deployment phase  \n",
      "this section will discuss pre -deployment ai safety , focusing on the need for regulatory due \n",
      "diligence, followed by  a discussion around model quality control  and software quality assurance , and \n",
      "finally highlighting the need for  formal  independent validation.  \n",
      " \n",
      "2.1. regulatory due diligence  \n",
      "there are several key considerations when deploying radiology ai models in clinical settings to \n",
      "ensure patient safety and effective care , from statutory regulation to professional standards and \n",
      "general desirability . one of these is regulatory due diligence, i.e. ensuring ai models meet relevant \n",
      "regulations and standards. for example, the fda or the eu’s mdr, has specific guidelines for the \n",
      "approval of medical devices, including ai -based systems. these can be  informally augmented by \n",
      "professional societies (e.g. american college of radiolog y, radiological society of north america , \n",
      "european college of radiology ), promoting best practice recommendations for ai usage and reporting  \n",
      "[7], [8] . any entity implementing these samd  solutions, from healthcare systems and hospitals to ai \n",
      "marketplaces, should conduct a thorough due diligence process to ensure all implemented models are \n",
      "safe, effective, and regulatory compliant.  \n",
      " \n",
      "the regulatory approval process for ai -based medical devices differs in complexity and rigor \n",
      "between countries, ranging from mostly procedural to mostly scientifically rigorous requirements, but \n",
      "all aim to validate the device development methodology, and a lgorithmic safety and efficacy through \n",
      "some form of premarket validation. manufacturers must submit detailed information to regulators on \n",
      "a plethora of aspects such as intended use, algorithm design, performance testing, risk analysis, quality \n",
      "management s ystems, and post -market surveillance. regulators reviewing  submissions may request \n",
      "additional information , such  as prospective clinical studies , to substantiate the performance of the ai \n",
      "model ; approval is often  based on reasonable assurances of safety and effectiveness for the intended \n",
      "use. many research groups, with the support of professional societies such as acr and rsna, have \n",
      "enhanced formal regulatory requirements  by publishing guidance documents such as stard -ai [9], \n",
      "future -ai [10], mi-claim [11] or minimar  [12], outlining principles and best practices for healthcare \n",
      "ai. these cover topics like appropriate clinical validation  and their evidence hierarchy , transparent \n",
      "reporting of capabilities and limitations,  and trial protocol design considerations . while  informal , \n",
      "these guidelines represent a consensus expert view and can serve as complements to formal \n",
      "regulations.  \n",
      " \n",
      " ai deployment platforms  also have an obligation to their users to ensure models listed on their \n",
      "platforms  meet all regulatory requirements . testing performed by platform providers should involve \n",
      "several crucial steps and should be geographically informed, as every market will have its own set of \n",
      "requirements. first, it requires confirming that the system holds the necessary certifications and \n",
      "clearances to operate legally within specific regions or countries, as regulatory requirements may \n",
      "differ across locations. secondly, it necessitates com pliance with registration obligations , by \n",
      "registering the a i system with relevant authorities or agencies . lastly , verifying the availability of \n",
      "comprehensive user -facing materials in relevant languages is required for effective and safe utilization \n",
      "of the system by healthcare professionals. these verification steps collectively contribute towards a \n",
      "thorough due diligence process before deploying a radiology ai system.   \n",
      " \n",
      " \n",
      "  \n",
      "2.2. independent model validation  \n",
      "external and independent model validation, either as an independent retrospective data analysis \n",
      "for self-contained models, or  preferably, a standalone prospective randomised controlled trial \n",
      "studying end -to-end solution value, can provide reassurances on model performance, calibration, \n",
      "value, and risk. by combining the findings of multiple smaller independent studies into a single meta -\n",
      "study , one can often gather more comprehensive evidence than the original validation set used for \n",
      "regulatory approval.  \n",
      "while regulatory agencies require that ai algorithms used in medical devices undergo rigorous \n",
      "testing and validation prior to regulatory approval for clinical use, a broadly comprehensive validation \n",
      "is hard to attain . this is especially important given that the clinical validation of ai models required for \n",
      "regulatory clearance is , in retrospect, often limited in scale and diversity. many models  are tested on \n",
      "curated limited  datasets that may not fully capture real -world variability across different hospitals, \n",
      "scanners, and populations, motivating the need for supplemental external validation studies that can \n",
      "be designed to focus on diverse challenging cases missed during init ial validation. the degree and \n",
      "hierarchy of evidence in ai validation studies  is thus of great importance.   \n",
      "while independent validation studies can be conducted by ai-deploying hospitals, research \n",
      "institutions, ai marketplace companies, or medical societies, meta -analyses of multiple small studies \n",
      "can provide additional performance insights. the aggregated results of a meta -study can identify a \n",
      "model’s particular areas of struggl e, providing insights towards generalizability and enabling the users \n",
      "to better understand the appropriate usage context and limitations of an ai model . it is recommended \n",
      "that ai model a nd platform vendors streamline the process of independent validation by potential \n",
      "client institutions , both  via p urpose -specific platforms and by enabling broad data access , e.g. via \n",
      "federated learning  [13]. standardized technical support for quick deployment and monitoring during \n",
      "evaluation trials with transparent data use  agreements can be beneficial for all parties.  \n",
      "insights from these independent validation efforts should even be shared with regulatory bodies \n",
      "to aid in continued oversight and to build the body of evidence needed for safe ai usage. model \n",
      "manufacturers also benefit from the feedback to improve their al gorithms or deployment \n",
      "recommendations. most importantly, such supplemental validation , primarily when used in a \n",
      "continuous and ongoing manner,  helps maintain trust and confidence in the ai model among end -\n",
      "users.  \n",
      " \n",
      "2.3. model quality control  \n",
      "another important aspect of ai deployment is to ensure models perform well at each hospital they \n",
      "are deployed during the commissioning process. this includes validating and documenting the \n",
      "accuracy and reliability of a model using hospital and population -specific real -world datasets.  \n",
      " \n",
      "often ai models are developed and tested using limited datasets that may not fully represent real -\n",
      "world diversity [14]. hence , locally validating models before deployment is crucial. ideally, one should \n",
      "curate a representative dataset from the hospital covering different patient demographics, scanner \n",
      "models, pathologies, etc, with the ai model then being evaluated on this dataset  to gauge real -world \n",
      "performance.  this is particularly important as disease prevalence and phenotype can change \n",
      "significantly worldwide, e.g. lung nodule subtype prevalence  differ s significantly between countr ies \n",
      "and regions. [15]  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "figure 2 - diagram depicting differences in population spread between  classical high -quality  selected  training  that matches \n",
      "inclusion criteria , and highly heterogeneous unselected target populations  with complex clinical presentations . \n",
      " \n",
      "quantitative metrics like accuracy, sensitivity, specificity, auc, as well as a qualitative clinical \n",
      "review of model outputs, are important at the pre -deployment quality control (qc) stage.  a site -\n",
      "specific qc step also allows the evaluation of different op erating thresholds, which can optimise trade -\n",
      "offs between metrics (e.g. sensitivity/specificity) for a target population.   \n",
      "it is important to note that this quality control check can be either algorithmic or based on human \n",
      "experts, and should target model inputs (e.g. does the input data look like what is expected), model \n",
      "outputs (i.e. do the model outputs make sense given an input), and population level statistics (i.e. is \n",
      "the model well calibrated at the population level). while human -based quality control for all ai models \n",
      "and sites would be ideal, it is often non -scalable due to limited clinical time availability and cost. new \n",
      "algorithmic solutions for input quality control [16], input domain shift [17], output quality [18] and \n",
      "model calibration [19] will help standardize , automate,  and make this qc step more scalable.  \n",
      " \n",
      "2.4. software quality assurance  \n",
      "the software technology stack used to develop radiology ai models must  be of high quality and \n",
      "meet certain standards for safety and efficacy, ideally building upon trusted open -source toolkits such \n",
      "as monai  [20]. when ai medical device manufacturers package these ai models into a complete \n",
      "samd , they need to ensure , to the best of their ability, that  their software  is free from errors, bugs, \n",
      "and security vulnerabilities, and that it can handle large amounts of data. while most quality assurance \n",
      "(qa) processes are implemented by the manufacturer itself, bugs and errors in ai models are often a \n",
      "function of the input d ata, and thus models need to be  continuously qa tested at every site they are \n",
      "deployed to ensure that ai models are reliable and effective.  \n",
      " \n",
      "like any other software, new errors and defects can appear during installation and deployment. \n",
      "common issues, such as unexpected algorithmic failures (e.g. high -resolution image causes failure due \n",
      "to lack of compute memory), incorrect inputs (e.g. unsuppor ted data geometry), and slow \n",
      "performance (e.g. particular data taking too much time to process) require rigorous software  qa \n",
      "testing to identify these before deployment. while manufacturers conduct qa during development, \n",
      "additional testing (in real -world s ettings) should be done after integrating the software with the \n",
      "hospital it systems. differences in infrastructure, data formats, and usage patterns can cause new \n",
      "issues to surface. qa testing the deployment platform with real hospital data at scale is adv isable \n",
      "before live availability. it is also important to load,  and stress test all models to evaluate their \n",
      "performance and scalability under heavy usage, and to have a clearly defined process for resolution \n",
      "based on issue severity and associated service l evel agreements.  \n",
      "heter ogeneous t arget population\n",
      "selective t raining population \n",
      " \n",
      " \n",
      "  \n",
      "3. production deployment  monitoring phase  \n",
      "this section  will discuss different types of ai safety  during production deployment , ensuring input and \n",
      "output data is appropriately controlled for, that algorithms can know what they don’t know , and that \n",
      "algorithmic execution is appropriately monitored.   \n",
      " \n",
      "3.1.  input quality control  \n",
      "input data quality is of paramount importance when ensuring ai models are safe and accurate. \n",
      "while data quality checks are often done by ai manufacturers themselves as part of their pre -\n",
      "deployment q a , most models depend on some system -level control to ensure data quality. for \n",
      "example, in a radiology practice , the imaging examination should be of the right modality, have  the \n",
      "correct organ in the field of view and be of sufficient diagnostic  quality for further downstream \n",
      "analysis so that it can be correctly processed by an ai model and generate the correct output. real -\n",
      "time  quality control mechanisms should be in place  to ensure that input data distribution , or the task \n",
      "concept itself, does not drift from the one used during  pre-deployment ai quality control, as model \n",
      "performance would not otherwise be guaranteed.  \n",
      " \n",
      "ai models are very sensitive to the quality and format of input data they receive. issues with input \n",
      "data are a common cause of unreliable model performance after deployment. more specifically, live \n",
      "input quality can be monitored through:  \n",
      " \n",
      "● data type checking  - confirm inputs match expected modalities (x -ray, ct, mri etc.)  \n",
      "● metadata validation  - data should have dicom tags indicating the imaged body part. inputs \n",
      "should match the model's intended use and anatomy.  \n",
      "● acquisition parameter checks  - confirm protocol parameters like scanner acquisition \n",
      "characteristics, image resolution, and use of contrast are within specified bounds.  \n",
      "● quality assessment  - measure noise levels, motion artifacts, foreign bodies, contrast levels \n",
      "etc. and check against  prescribed threshold ranges from the ai vendor.  \n",
      " \n",
      "both programmatic checks and human review are useful here: input quality issues can trigger alerts \n",
      "for technicians to rectify acquisition, and population shifts can require a new quality control step. \n",
      "unsurprisingly , the same class of algorithms used to automate pre -deployment qc (sec 2.3) such as \n",
      "input quality control [sadri et al. 202] and input domain shift [graham et al. 2022 pmlr] can be \n",
      "repurposed for live deployment and continuous monitoring.  summarily, high -quality and trusted \n",
      "inputs are the bedroc k for reliable ai model performance.  \n",
      " \n",
      "3.2. output guardrails  \n",
      "when an ai model is presented with data similar to the one the model has seen during training, \n",
      "performance should be comparable to the one observed during model development , as the model is \n",
      "“in-distribution ”. however, in order to further minimize ai risks at the individual patient level, some \n",
      "ai manufacturers implement additional output guardrail mechanisms in the form of model confidence  \n",
      " \n",
      " \n",
      " or uncertainty estimation [mehrtash et al. 2020 ieee tmi], explicitly explainable models that can then \n",
      "be logically controlled for, or separate redundant quality control systems [valindria et al. 2017 tmi]. \n",
      "however, as these output guardrail mechanisms are  only seldom in place, a platform -level redundancy \n",
      "mechanism may be necessary to ensure safety across all deployed ai models. it is also important that \n",
      "these failures are appropriately recorded and communicated downstream so that patients can be \n",
      "safely car ed for. while input quality control aims to ensure models receive appropriate data, output \n",
      "guardrails monitor the models' actual performance on those inputs, with approaches depicted in fig \n",
      "4 below, such as:  \n",
      " \n",
      "● uncertainty estimation  – models predict an uncertainty score along with each output \n",
      "signifying its confidence. outputs with high uncertainty can be flagged for review.  \n",
      "● consistency  - passing the same input to multiple independent models (for the same task) and \n",
      "checking if their predictions agree can help detect errors.  \n",
      "● logical constraints  – outputs are checked against logical rules and constraints , e.g. for an \n",
      "estimated volume being within an expected range. violations are flagged.  \n",
      "● distribution monitoring  – the distribution of model outputs is tracked for drift over time. \n",
      "sudden distribution shifts could indicate a problem.  \n",
      "● human -in-the-loop  – a certain percentage of outputs could be validated by human expert \n",
      "reviewers. disagreements indicate poor performance.  \n",
      " \n",
      " \n",
      " \n",
      "figure 4 - depiction of different types of system level safety mechanisms  \n",
      "the platform should define operating thresholds for these checks based on model specifics and \n",
      "risk tolerance. detected issues should trigger alerts to the underlying ai medical device manufacturer,  \n",
      "along with deferring the results from getting to clinical users. this allows for proactive debugging and \n",
      "re-training of models if needed.  \n",
      " \n",
      "3.3. deferral of algorithmic decisions  \n",
      "with the introduction of platform -level input and output guardrail mechanisms, it is important to \n",
      "ensure these acts  as a fail -safe. if an algorithmic output is deemed not to be of sufficient quality \n",
      "according to a locally calibrated threshold, it should be deferred to the most appropriate downstream \n",
      "expert clinician while providing a transparent explanation for the def erral reason. it is important to \n",
      "note that while deferral mechanisms  do not fully mitigate but minimis e risk in a medical device . such \n",
      "a defe rral mechanism should also be integrated with the patient's electronic health record or the \n",
      "radiological information system to ensure that all patients are appropriately cared for regardless of \n",
      "the reason that triggered the deferral.  \n",
      " \n",
      "when model outputs fail guardrail checks, they cannot be shown to users and must be deferred to \n",
      "human experts for review. however, deferring directly can lead to patient care delays, gaps in the \n",
      " \n",
      " \n",
      " \n",
      " workflow, or additional work burden on clinical staff. the deferral mechanism should therefore \n",
      "seamlessly integrate with existing clinical reporting systems , adhere to interoperability standards and \n",
      "profiles , and automatically assign deferred cases to and notify radiologists based on configurable rules \n",
      "accounting for sub -specialization, availability etc. it should also track deferred case workload at the \n",
      "platform and individual user levels for analytics and provide  auditable logging of all deferrals with clear \n",
      "reasons linked to the patient record. with tight integration into existing systems and clinical \n",
      "workflows, deferred cases due to guardrail failures can be handled gracefully avoiding gaps in patient \n",
      "care.  \n",
      " \n",
      "3.4. model execution  monitoring  \n",
      "lastly, as ai models are ultimately software, it is important that we track key performance \n",
      "indicators related to model runtime, failure and deferral rates, accuracy, precision, and recall, so these \n",
      "can be audited and managed. if an unexpected failure occu rs, or the execution time  of the models is \n",
      "beyond the expected service level agreement, a system administrator needs to be notified via an \n",
      "incident management system. it is also important that all model executions are appropriately logged \n",
      "in a harmonized  and well -documented manner for post -hoc analysis and quality assurance purposes. \n",
      "in addition to monitoring model inputs and outputs, tracking technical metrics around the model \n",
      "execution itself is also important:  \n",
      " \n",
      "● model versioning  - link executions to specific model versions to track provenance.  \n",
      "● runtimes  - monitor latency, throughput and model inference times  of the ai application , and \n",
      "flag violations of service agreements.  \n",
      "● hardware/resource utilization  - track gpu/ram usage spikes indicating performance issues.  \n",
      "● errors/failures  - log any unexpected errors like gpu out-of-memory failures.  \n",
      "● audit logs  - detailed execution logging for traceability and recreation of issues.  \n",
      "● deferral rates  - measure overall deferral rates as well as reason -wise breakdown.  \n",
      "● accuracy metrics  - log model outputs and eventual clinician diagnoses  and dissent  to track \n",
      "accuracy over time.  \n",
      " \n",
      "these metrics provide insights into model performance in production and can alert to regressions, \n",
      "and debugging data enables root cause analysis of problems by ai teams as long -term metric trends \n",
      "can identify needs for re -training or upgrade. detailed logs  are also crucial for regulatory compliance \n",
      "and audits. execution monitoring, together with the input and output guardrails, provides full and \n",
      "comprehensive visibility into ai systems.  \n",
      " \n",
      "4. post -market surveillance phase  \n",
      " \n",
      "4.1. performance drift monitoring  \n",
      "algorithmic guardrails and monitoring during live ai deployment allow the identification of subject -\n",
      "level failures and problems, but many effects are only visible at the population level. for example, \n",
      "algorithmic performance might decay over time due to ch anges in how the data are acquired, updating  \n",
      " \n",
      " \n",
      " of imaging scanners and sequences, or phenotype/population drift effects. it is thus important to track \n",
      "and audit model performance ; this can be achieved algorithmically, via manual clinical review, or by \n",
      "comparing with downstream clinical diagnosis and outcomes as recorded in the electronic health \n",
      "record. if performance degradation is observed, a full clinical review of the problem, and subsequently \n",
      "redoing the quality control study, will be necessary to avoid further impacting patient care.  \n",
      " \n",
      "ai model performance can slowly drift from validated levels post -deployment due to a variety of \n",
      "reasons, from data drift, i.e. gradual changes in input data characteristics like resolution, contrast, or \n",
      "new scanners/protocols; to phenotype drift, i.e. changes in disease morphology and presentation over \n",
      "time; and concept drift, i.e. evolution in best practices and diagnostic standards.  it is also important \n",
      "to note that deploying ai might create degenerate feedback loops between ai recommendations and \n",
      "outcome s [21], further drifting performance.  \n",
      " \n",
      "continuously monitoring metrics like accuracy, sensitivity, and specificity versus time can detect \n",
      "such gradual performance degradation. comparing model outputs against clinician diagnoses over a \n",
      "period of time provides another signal of drift.  \n",
      " \n",
      "upon suspected drift, a retrospective analysis of model errors should be undertaken. additional \n",
      "blinded validation by clinical experts on a recent dataset would provide a gold standard quantif ication \n",
      "of the extent of degradation. based on the findings, the model may need retraining on suitable \n",
      "additional data , and adjustment of operating thresholds , or model retirement . being proactive about \n",
      "monitoring and addressing drift is crucial to prevent a gradual decline in model utility over time.  \n",
      " \n",
      "4.2. model recalibration  \n",
      "similarly,  to performance drift, algorithms often make predictions according to either \n",
      "development -stage or pre -deployment stage calibration of the operating point of a receiver -operating \n",
      "curve (roc), targeting a specific sensitivity or specificity depending on the clinical use case. for \n",
      "example, screening and assessment stages of breast cancer diagnosis requires  different \n",
      "sensitivity/specificity operating points.  as data and models drift, it is often necessary to recalibrate \n",
      "the roc curve operating point to ensure t he clinically ideal behaviour  of the model , which is a function \n",
      "of regional  and/or physician preference . by tracking population -level metrics over time and \n",
      "conducting audits, degrading sensitivity or specificity can be identified. the current threshold and \n",
      "desired levels can be provided to ai teams to re -calibrate the models accordingly. even though this \n",
      "process can be automated [cite ben], it is important to note that fully automated continuous \n",
      "recalibration is currently infeasible in certain geographies d ue to regulations. however, having \n",
      "processes to periodically adjust thresholds in consultation with medical leaders ensures optimal \n",
      "clinical utility. overall, tracking algorithmic sensitivity and specificity over time and triggering a review \n",
      "process is nec essary to ensure ai models behave appropriately.  \n",
      " \n",
      "4.3. bias and fairness  \n",
      "ai safety is often perceived as either a population -wide or a subject -specific task, but several \n",
      "harmful algorithmic effects can occur in subsets of the overall population. from a bias and fairness \n",
      "point of view, and if ai is to be trusted by all members o f society, it is important to verify that model \n",
      "performance is equitable across different population subgroups, primarily between clusters of  \n",
      " \n",
      " \n",
      " subjects that share protected characteristics. if sufficient evidence of  sub-population  \n",
      "underperform ance is found (including both protected characteristics and phenotypical  variance ), a full \n",
      "clinical review should be conducted to assess the impact on those populations , followed by an  \n",
      "appropriate  algorithmic  remediation  and notifi cation of both manufacturer and regulators  of this \n",
      "adverse finding.  \n",
      "while overall metrics may show a model is safe , effective  and efficacious,  it can still \n",
      "disproportionately underperform in certain population segments. unfairness can be caused by \n",
      "underrepresentation in training data, where a model may not have seen enough examples from that \n",
      "population to generalise well, proxies and correlation s, where a model may learn to depend on \n",
      "covariates that correlate with sensitive attributes, and concept drift, where disease patterns and \n",
      "clinical standards for different population s may differ.  remediation strategies can be introduced \n",
      "during p re-processing (e.g. targeted data collection), in -processing (e.g. adversarial learning), and \n",
      "post -processing (calibration).  \n",
      "it is also important to note that u nfair model performance, if found, should warrant detailed \n",
      "disclosure to providers , patients and carers from that community , as collaborating with leaders from \n",
      "the community on mitigation approaches will contribute to build ing trust. efforts to improve model \n",
      "equity should also be undertaken, including gathering more representative data.  \n",
      " \n",
      "4.4. incident tracking  \n",
      "regulators increasingly require  manufacturers to establish an incident tracking system as part of a \n",
      "post market surveillance system to proactively collect and review experience gained from devices that \n",
      "are placed on the market, with the aim of identifying any need to immediately apply corrective or \n",
      "preventive action s. a key component is maintaining an incident tracking process to record issues on \n",
      "device performance or safety , including model provenance, data hashes, hardware configuration and \n",
      "integration to oling, that come to the manufacturer's attention after market release. incident data \n",
      "helps identify software anomalies, or use errors not caught during pre -deployment testing. rapidly \n",
      "detecting and addressing incidents is not only a legal obligation, but also vit al for managing risks and \n",
      "ensuring the ai technology remains safe and functions as intended post deployment.  \n",
      " \n",
      "4.5. value tracking  \n",
      "value -based healthcare [22] is a model where providers are reimbursed based on the value of care \n",
      "they deliver, rather than the volume of services provided. ai safety is thus interlinked with value -based \n",
      "healthcare  [23], not only because a simple ai model decision can have long -term implications in the \n",
      "outcome and thus the value of the care delivered to patients, but also because a better understanding \n",
      "of the value created by a model would allow for a more informed view of when and to which degree \n",
      "a model should be used in a clinical setting.  there also is a disparity between vendo r and patient  \n",
      "incentives , as, for example,  an ai vendor might prefer a higher case throughput  while a specific patient \n",
      "might not benefit from the increased cost . overall, while a better understanding of the value  of an ai \n",
      "model does not necessarily contribute towards ai safety, it does significantly contribute towards trust \n",
      "in the technology itself, primarily from the point of view of patients and their carers.  \n",
      "as value -based care focuses on optimizing  patient outcomes while lowering costs, if ai is to be \n",
      "made safe but also trustworthy, it is important to capture the different sources of the ai value chain, \n",
      "such as enabling earlier and more accurate diagnoses leading to timely intervention, reducing \n",
      "unnecessary diagnostic testing in obvious negative cases, and automating tedious workflows  \n",
      " \n",
      " \n",
      " improving clinician productivity.  however, unsafe or poorly performing ai can negatively impact value \n",
      "through inaccurate diagnoses , prognosis, planning, among other use cases,  resulting in improper \n",
      "treatment plans, loss of trust in ai leading to ignoring valid model outputs by clinicians, and liability \n",
      "costs and reputational damage from adverse events involving ai. evaluating the impact of ai on \n",
      "metrics like clinical outcomes, patient satisfaction, clinical workflow integration, operational \n",
      "efficiency, and costs is crucial. overall, a virtuous cycle of deploying ai safety initiatives to build \n",
      "provider trust is needed, thus enabling expanded use of ai, and leading to greate r value delivery.  \n",
      "5. conclusion  \n",
      "the deployment of ai systems in radiology holds great potential to improve clinical care, but also \n",
      "carries risks if not implemented thoughtfully. as outlined in this paper, a holistic approach to \n",
      "trustworthy ai that encompasses the entire product lifecycle  is crucial.  \n",
      "rigorous pre -deployment evaluation and validation ensure ai models are safe and effective for the \n",
      "intended use case. input and output guardrails during live usage provide an additional safety net to \n",
      "identify individual failures. continuous monitoring after  deployment enables tracking of population -\n",
      "level performance, fairness,  and value delivery over time.  \n",
      "no one solution alone can guarantee absolute safety. rather, it is the synergistic application of \n",
      "quality assurance at multiple levels - regulatory, clinical, technical,  and ethical - that fosters \n",
      "appropriate trust in ai. collaboration between stakeholders across healthcare systems, industry, \n",
      "academics,  and government is vital to address the interdisciplinary challenges involved.  \n",
      "trust is earned, not given. by transparently demonstrating that ai is held to the same safety and \n",
      "efficacy standards as other medical technologies, developers can build confidence among providers \n",
      "and patients. this will enable the responsible scaling of ai  and realization  of its potential benefits. the \n",
      "roadmap presented here aims to accelerate this goal of deployable, reliable,  and safe ai in radiology.  \n",
      " \n",
      "funding/disclosures  \n",
      "m. j. cardoso  and s. ourselin are funded by the wellcome programme on high -dimensional neurology \n",
      "(wt213038/z/18/z) and wellcome epsrc cme (wt203148/z/16/z).  t. lee is funded by the \n",
      "engineering & physical sciences research council doctoral training partnership (epsrc dtp) grant \n",
      "ep/t517963/1 . r. m. summers receives royalties from icad, philips, scanmed, pingan, translation \n",
      "holdings and mgb. he has received research support from pingan. this work is funded in part by the \n",
      "intramural research program of the nih clinical center. the opinions expr essed herein are those of \n",
      "the aut hors and not necessarily those of the u.s. department of health and human services or the \n",
      "national institutes of health.  \n",
      " \n",
      " \n",
      "bibliography  \n",
      "[1] joseph r. biden jr, executive order on the safe, secure, and trustworthy \n",
      "development and use of artificial intelligence . 2023. accessed: nov. 21, 2023. \n",
      "[online]. available: https://www.whitehouse.gov/briefing -room/presidential -\n",
      "actions/2023/10/30/executive -order -on-the-safe-secure -and-trustworthy -development -\n",
      "and-use-of-artificial -intelligence/   \n",
      " \n",
      " \n",
      " [2] european commission, proposal for a regulation of the european parliament and of \n",
      "the council laying down harmonised rules   on artificial intelligence (artificial \n",
      "intelligence act) and amending certain union legislative acts . european commission: \n",
      "https://eur -lex.europa.eu/legal -content/en/txt/?uri=celex:52021pc0206, 2021. \n",
      "accessed: nov. 21, 2023. [online]. available: https://eur -lex.europa.eu/legal -\n",
      "content/en/txt/?uri=celex:52021pc0206  \n",
      "[3] fda, ‘software as a medical device (samd): clinical evaluation - guidance for \n",
      "industry and food and drug administration staff’, 2017. accessed: nov. 17, 2023. \n",
      "[online]. available: https://www.fda.gov/regulatory -information/search -fda-guidance -\n",
      "documents /software -medical -device -samd -clinical -evaluation  \n",
      "[4] fda, ‘artificial intelligence and machine learning in software, u.s. food and drug \n",
      "administration’, https://www.fda.gov/medical -devices/software -medical -device -\n",
      "samd/artificial -intelligence -and-machine -learning -software -medical -device . accessed: \n",
      "nov. 1 7, 2023. [online]. available: https://www.fda.gov/medical -devices/software -\n",
      "medical -device -samd/artificial -intelligence -and-machine -learning -software -medical -\n",
      "device  \n",
      "[5] v. gupta et al. , ‘current state of community -driven radiological ai deployment in \n",
      "medical imaging’, dec. 2022.  \n",
      "[6] h. bosmans, f. zanca, and f. gelaude, ‘procurement, commissioning and qa of ai \n",
      "based solutions: an mpe’s perspective on introducing ai in clinical practice’, physica \n",
      "medica , vol. 83, pp. 257 –263, mar. 2021, doi: 10.1016/j.ejmp.2021.04.006.  \n",
      "[7] m. p. sendak, m. gao, n. brajer, and s. balu, ‘presenting machine learning model \n",
      "information to clinical end users with model facts labels’, npj digit med , vol. 3, no. 1, \n",
      "p. 41, mar. 2020, doi: 10.1038/s41746 -020-0253 -3. \n",
      "[8] x. liu et al. , ‘reporting guidelines for clinical trial reports for interventions involving \n",
      "artificial intelligence: the consort -ai extension’, nat med , vol. 26, no. 9, pp. \n",
      "1364 –1374, sep. 2020, doi: 10.1038/s41591 -020-1034 -x. \n",
      "[9] v. sounderajah et al. , ‘developing a reporting guideline for artificial intelligence -\n",
      "centred diagnostic test accuracy studies: the stard -ai protocol’, bmj open , vol. 11, \n",
      "no. 6, p. e047709, jun. 2021, doi: 10.1136/bmjopen -2020 -047709.  \n",
      "[10] k. lekadir et al. , ‘future -ai: international consensus guideline for trustworthy and \n",
      "deployable artificial intelligence in healthcare’, aug. 2023.  \n",
      "[11] b. norgeot et al. , ‘minimum information about clinical artificial intelligence \n",
      "modeling: the mi -claim checklist’, nat med , vol. 26, no. 9, pp. 1320 –1324, sep. \n",
      "2020, doi: 10.1038/s41591 -020-1041 -y. \n",
      "[12] t. hernandez -boussard, s. bozkurt, j. p. a. ioannidis, and n. h. shah, ‘minimar \n",
      "(minimum information for medical ai reporting): developing reporting standards \n",
      "for artificial intelligence in health care’, journal of the american medical informatics \n",
      "association , vol. 27, no. 12, pp. 2011 –2015, dec. 2020, doi: 10.1093/jamia/ocaa088.  \n",
      "[13] n. rieke et al. , ‘the future of digital health with federated learning’, npj digit med , \n",
      "vol. 3, no. 1, p. 119, sep. 2020, doi: 10.1038/s41746 -020-00323 -1. \n",
      "[14] a. youssef, m. pencina, a. thakur, t. zhu, d. clifton, and n. h. shah, ‘external \n",
      "validation of ai models in health should be replaced with recurring local validation’, \n",
      "nat med , oct. 2023, doi: 10.1038/s41591 -023-02540 -z. \n",
      "[15] d. e. dawe, h. singh, l. wickramasinghe, m. w. pitz, and m. torabi, ‘geographical \n",
      "variation and factors associated with non -small cell lung cancer in manitoba’, can \n",
      "respir j , vol. 2017, pp. 1 –9, 2017, doi: 10.1155/2017/7915905.  \n",
      "[16] a. r. sadri et al. , ‘mrqy — an open‐source tool for quality control of mr imaging \n",
      "data’, med phys , vol. 47, no. 12, pp. 6029 –6038, dec. 2020, doi: 10.1002/mp.14593.   \n",
      " \n",
      " \n",
      " [17] m. s. graham et al. , ‘latent transformer models for out -of-distribution detection’, \n",
      "med image anal , vol. 90, p. 102967, dec. 2023, doi: 10.1016/j.media.2023.102967.  \n",
      "[18] v. v. valindria et al. , ‘reverse classification accuracy: predicting segmentation \n",
      "performance in the absence of ground truth’, ieee trans med imaging , vol. 36, no. \n",
      "8, pp. 1597 –1606, aug. 2017, doi: 10.1109/tmi.2017.2665165.  \n",
      "[19] m. roschewitz et al. , ‘automatic correction of performance drift under acquisition \n",
      "shift in medical image classification’, nat commun , vol. 14, no. 1, p. 6608, oct. 2023, \n",
      "doi: 10.1038/s41467 -023-42396 -y. \n",
      "[20] m. j. cardoso et al. , ‘monai: an open -source framework for deep learning in \n",
      "healthcare’, nov. 2022.  \n",
      "[21] r. jiang, s. chiappa, t. lattimore, a. györgy, and p. kohli, ‘degenerate feedback \n",
      "loops in recommender systems’, feb. 2019, doi: 10.1145/3306618.3314288.  \n",
      "[22] m. e. porter and e. o. teisberg, redefining health care: creating value -based \n",
      "competition on results . harvard business review press, 2006. [online]. available: \n",
      "https://books.google.co.uk/books?id=cse2loandnic  \n",
      "[23] p. esmaeilzadeh, ‘use of ai -based tools for healthcare purposes: a survey study from \n",
      "consumers’ perspectives’, bmc med inform decis mak , vol. 20, no. 1, p. 170, dec. \n",
      "2020, doi: 10.1186/s12911 -020-01191 -1. \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text().lower()\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Print the text extracted from the PDF\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Chunk 1: This is a list of contributors from various institutions including King's College London, Deepc GmbH, University of Pennsylvania, Mayo Clinic, NVIDIA, and Vanderbilt University. They have all equally contributed to a project or research focused on Radiology AI safety, employing an end-to-end lifecycle approach.\n",
      "\n",
      "Summary for Chunk 2: The integration of AI into radiology offers opportunities for improved clinical care and efficiency but also requires a careful approach to manage potential risks. Models should meet high safety, effectiveness, and efficacy standards, with rigorous pre- and post-deployment evaluation. Continuous monitoring, input and output guardrails, and education of radiologists on new AI-driven findings are crucial. Addressing these challenges requires collaborative efforts between healthcare systems, industry, academia, and government. Trust in AI is earned through transparent demonstration of adherence to rigorous safety and efficacy standards, instilling confidence among providers and patients. The article also emphasizes the importance of regulatory compliance, stating that meeting regulatory standards is a legal obligation for healthcare AI. The paper proposes a life-cycle approach to AI deployment, tackling pre-, peri-, and post-deployment stages. This includes due diligence, extensive quality assurance testing, independent validation studies, live monitoring during deployment, and post-deployment surveillance.\n",
      "\n",
      "Summary for Chunk 3: The content discusses the pre-deployment phase of a proposed Radiology AI safety lifecycle approach. It highlights the importance of regulatory due diligence, ensuring AI models meet relevant regulations and standards, such as those set by the FDA or the EU's MDR. It also mentions the necessity for independent model validation and model quality control. Regulatory due diligence involves thorough checks to ensure models are safe, effective, and regulatory compliant. The process varies in complexity between countries, but aims to validate the device development methodology and algorithmic safety. Independent model validation provides reassurances on model performance, value, and risk. It emphasizes the need for external validation studies, especially considering that models are often tested on curated limited datasets, which may not capture real-world variability. Model quality control ensures models perform well at each hospital they are deployed. This includes validating and documenting a model's accuracy and reliability using hospital and population-specific real-world datasets. Quality control check can be either algorithmic or based on human experts, and should target model inputs, outputs, and population level statistics. The content also stresses the importance of transparent reporting of capabilities and limitations, and the need for comprehensive user-facing materials in relevant languages.\n",
      "\n",
      "Summary for Chunk 4: The software technology stack used in radiology AI models must be of high quality, meeting safety and efficacy standards, and ideally based on trusted open-source toolkits like Monai. When AI medical device manufacturers create a complete Software as a Medical Device (SaMD), they need to ensure the software is free from errors, bugs, and security vulnerabilities. Quality Assurance (QA) processes are usually implemented by the manufacturer, but due to the potential for model errors based on input data, continuous QA testing at every deployment site is necessary. Common issues during installation and deployment, like unexpected algorithmic failures, incorrect inputs, and slow performance, require rigorous QA testing. Additional testing in real-world settings should be done after integrating the software with hospital IT systems. Load and stress testing all models is also recommended to assess performance and scalability under heavy usage. The input data quality is crucial for safe and accurate AI models. Real-time quality control mechanisms should be in place to ensure task concept or data distribution doesn't drift from the pre-deployment AI quality control. AI models are very sensitive to the quality and format of their input data. Live input quality can be monitored through data type checking, metadata validation, acquisition parameter checks, and quality assessment. Both programmatic checks and human review are helpful. To minimize AI risks at the individual patient level, some AI manufacturers implement additional output guardrail mechanisms. These include model confidence or uncertainty estimation, explicitly explainable models, or separate redundant quality control systems. Input quality control ensures models receive appropriate data, while output guardrails monitor the models' actual performance on those inputs.\n",
      "\n",
      "Summary for Chunk 5: The content discusses the importance of system level safety mechanisms in AI medical devices. These mechanisms should define operating thresholds based on model specifics and risk tolerance. When issues are detected, they should trigger alerts to the device manufacturer and defer the results from reaching clinical users, allowing for proactive debugging and possible re-training of models. There's a need for a deferral mechanism for algorithmic decisions when the output is not of sufficient quality. This mechanism should be integrated with the patient's electronic health record to ensure all patients are cared for appropriately. When model outputs fail checks, they should be deferred to human experts for review. The deferral mechanism should integrate with existing clinical reporting systems and automatically assign deferred cases to radiologists based on various factors. Additionally, the performance of AI models should be tracked through key indicators like model runtime, failure and deferral rates, accuracy, precision, and recall. Monitoring includes model versioning, runtimes, hardware utilization, error tracking, audit logs, deferral rates, and accuracy metrics. Lastly, the content recommends post-market surveillance including performance drift monitoring to track and audit model performance. If performance degradation is observed, a full clinical review will be necessary to avoid further impacting patient care.\n",
      "\n",
      "Summary for Chunk 6: The performance of AI models can drift from validated levels post-deployment due to various factors such as data drift, phenotype drift, and concept drift. This could potentially create feedback loops between AI recommendations and outcomes, further affecting the performance. By continuously monitoring metrics like accuracy, sensitivity, and specificity, this gradual degradation can be detected and addressed. Upon suspecting drift, a retrospective analysis should be done, and additional validation by clinical experts on a recent dataset could provide quantification of degradation. The model might need retraining or adjustments based on these findings. AI models might also need recalibration as data and models drift. For instance, the operating point of a receiver-operating curve (ROC) might need adjustment to ensure the model behaves ideally. It's also critical to ensure AI model performance is equitable across different population subgroups. If a model underperforms for certain subsets, a full clinical review should be conducted, followed by appropriate algorithmic remediation. Regulators require manufacturers to establish an incident tracking system to proactively collect and review experience gained from devices on the market, aiming to identify any need for corrective or preventive actions. Lastly, AI safety is interlinked with value-based healthcare. Understanding the value created by a model would allow for a more informed view of when and to which degree a model should be used in a clinical setting. In conclusion, a holistic approach to trustworthy AI that encompasses the entire product lifecycle is crucial. No one solution can guarantee absolute safety, but it's the synergistic application of quality assurance at multiple levels that fosters appropriate trust in AI. Transparently demonstrating that AI is held to the same safety and efficacy standards as other medical technologies can help build confidence among providers and patients.\n",
      "\n",
      "Summary for Chunk 7: This bibliography includes a variety of sources related to the use of artificial intelligence (AI) in healthcare. Key documents include an executive order by Joseph R. Biden Jr. on safe AI development and use [1], a proposal by the European Commission for harmonized rules on AI [2], and guidelines from the U.S. Food and Drug Administration on AI and machine learning in software [3,4]. There are also numerous academic articles on topics such as community-driven AI deployment in medical imaging [5], introducing AI in clinical practice [6], presenting machine learning model information to clinical end users [7], guidelines for clinical trial reports for AI interventions [8], reporting guideline for AI-centred diagnostic test accuracy studies [9], international consensus guideline for trustworthy AI in healthcare [10], minimum information about clinical AI modeling [11], developing reporting standards for AI in healthcare [12,14], and the future of digital health with federated learning [13]. Other sources discuss geographical variation and factors associated with non-small cell lung cancer [15], an open-source tool for quality control of MR imaging data [16], latent transformer models for out-of-distribution detection [17], predicting segmentation performance in the absence of ground truth [18], automatic correction of performance drift under acquisition shift in medical image classification [19], an open-source framework for deep learning in healthcare [20], degenerate feedback loops in recommender systems [21], creating value-based competition on results in healthcare [22], and use of AI-based tools for healthcare purposes from consumers' perspectives [23].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Now, proceed with chunking and summarization\n",
    "#chunk_size = 4000\n",
    "#chunks = [pdf_text[i:i+chunk_size] for i in range(0, len(pdf_text), chunk_size)]\n",
    "\n",
    "\n",
    "# Split the document into paragraphs based on double newline characters\n",
    "paragraphs = re.split(r'\\n\\s*\\n', pdf_text)\n",
    "\n",
    "# Group paragraphs into sets of three\n",
    "chunks = [paragraphs[i:i+10] for i in range(0, len(paragraphs), 10)]\n",
    "\n",
    "# Initialize an empty list to store the summaries\n",
    "summaries = []\n",
    "\n",
    "# Loop through each chunk and summarize using GPT-3\n",
    "for idx, chunk in enumerate(chunks, start=1):\n",
    "    chunk_text = '\\n\\n'.join(chunk)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following content: {chunk_text} \"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        max_tokens=1000,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    generated_summary = response.choices[0].message.content\n",
    "    generated_summary = ' '.join(generated_summary.split())\n",
    "    summaries.append(generated_summary)\n",
    "    print(f\"Summary for Chunk {idx}: {generated_summary}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a list of contributors from various institutions including King's College London, Deepc GmbH, University of Pennsylvania, Mayo Clinic, NVIDIA, and Vanderbilt University. They have all equally contributed to a project or research focused on Radiology AI safety, employing an end-to-end lifecycle approach.\n",
      "\n",
      "The integration of AI into radiology offers opportunities for improved clinical care and efficiency but also requires a careful approach to manage potential risks. Models should meet high safety, effectiveness, and efficacy standards, with rigorous pre- and post-deployment evaluation. Continuous monitoring, input and output guardrails, and education of radiologists on new AI-driven findings are crucial. Addressing these challenges requires collaborative efforts between healthcare systems, industry, academia, and government. Trust in AI is earned through transparent demonstration of adherence to rigorous safety and efficacy standards, instilling confidence among providers and patients. The article also emphasizes the importance of regulatory compliance, stating that meeting regulatory standards is a legal obligation for healthcare AI. The paper proposes a life-cycle approach to AI deployment, tackling pre-, peri-, and post-deployment stages. This includes due diligence, extensive quality assurance testing, independent validation studies, live monitoring during deployment, and post-deployment surveillance.\n",
      "\n",
      "The content discusses the pre-deployment phase of a proposed Radiology AI safety lifecycle approach. It highlights the importance of regulatory due diligence, ensuring AI models meet relevant regulations and standards, such as those set by the FDA or the EU's MDR. It also mentions the necessity for independent model validation and model quality control. Regulatory due diligence involves thorough checks to ensure models are safe, effective, and regulatory compliant. The process varies in complexity between countries, but aims to validate the device development methodology and algorithmic safety. Independent model validation provides reassurances on model performance, value, and risk. It emphasizes the need for external validation studies, especially considering that models are often tested on curated limited datasets, which may not capture real-world variability. Model quality control ensures models perform well at each hospital they are deployed. This includes validating and documenting a model's accuracy and reliability using hospital and population-specific real-world datasets. Quality control check can be either algorithmic or based on human experts, and should target model inputs, outputs, and population level statistics. The content also stresses the importance of transparent reporting of capabilities and limitations, and the need for comprehensive user-facing materials in relevant languages.\n",
      "\n",
      "The software technology stack used in radiology AI models must be of high quality, meeting safety and efficacy standards, and ideally based on trusted open-source toolkits like Monai. When AI medical device manufacturers create a complete Software as a Medical Device (SaMD), they need to ensure the software is free from errors, bugs, and security vulnerabilities. Quality Assurance (QA) processes are usually implemented by the manufacturer, but due to the potential for model errors based on input data, continuous QA testing at every deployment site is necessary. Common issues during installation and deployment, like unexpected algorithmic failures, incorrect inputs, and slow performance, require rigorous QA testing. Additional testing in real-world settings should be done after integrating the software with hospital IT systems. Load and stress testing all models is also recommended to assess performance and scalability under heavy usage. The input data quality is crucial for safe and accurate AI models. Real-time quality control mechanisms should be in place to ensure task concept or data distribution doesn't drift from the pre-deployment AI quality control. AI models are very sensitive to the quality and format of their input data. Live input quality can be monitored through data type checking, metadata validation, acquisition parameter checks, and quality assessment. Both programmatic checks and human review are helpful. To minimize AI risks at the individual patient level, some AI manufacturers implement additional output guardrail mechanisms. These include model confidence or uncertainty estimation, explicitly explainable models, or separate redundant quality control systems. Input quality control ensures models receive appropriate data, while output guardrails monitor the models' actual performance on those inputs.\n",
      "\n",
      "The content discusses the importance of system level safety mechanisms in AI medical devices. These mechanisms should define operating thresholds based on model specifics and risk tolerance. When issues are detected, they should trigger alerts to the device manufacturer and defer the results from reaching clinical users, allowing for proactive debugging and possible re-training of models. There's a need for a deferral mechanism for algorithmic decisions when the output is not of sufficient quality. This mechanism should be integrated with the patient's electronic health record to ensure all patients are cared for appropriately. When model outputs fail checks, they should be deferred to human experts for review. The deferral mechanism should integrate with existing clinical reporting systems and automatically assign deferred cases to radiologists based on various factors. Additionally, the performance of AI models should be tracked through key indicators like model runtime, failure and deferral rates, accuracy, precision, and recall. Monitoring includes model versioning, runtimes, hardware utilization, error tracking, audit logs, deferral rates, and accuracy metrics. Lastly, the content recommends post-market surveillance including performance drift monitoring to track and audit model performance. If performance degradation is observed, a full clinical review will be necessary to avoid further impacting patient care.\n",
      "\n",
      "The performance of AI models can drift from validated levels post-deployment due to various factors such as data drift, phenotype drift, and concept drift. This could potentially create feedback loops between AI recommendations and outcomes, further affecting the performance. By continuously monitoring metrics like accuracy, sensitivity, and specificity, this gradual degradation can be detected and addressed. Upon suspecting drift, a retrospective analysis should be done, and additional validation by clinical experts on a recent dataset could provide quantification of degradation. The model might need retraining or adjustments based on these findings. AI models might also need recalibration as data and models drift. For instance, the operating point of a receiver-operating curve (ROC) might need adjustment to ensure the model behaves ideally. It's also critical to ensure AI model performance is equitable across different population subgroups. If a model underperforms for certain subsets, a full clinical review should be conducted, followed by appropriate algorithmic remediation. Regulators require manufacturers to establish an incident tracking system to proactively collect and review experience gained from devices on the market, aiming to identify any need for corrective or preventive actions. Lastly, AI safety is interlinked with value-based healthcare. Understanding the value created by a model would allow for a more informed view of when and to which degree a model should be used in a clinical setting. In conclusion, a holistic approach to trustworthy AI that encompasses the entire product lifecycle is crucial. No one solution can guarantee absolute safety, but it's the synergistic application of quality assurance at multiple levels that fosters appropriate trust in AI. Transparently demonstrating that AI is held to the same safety and efficacy standards as other medical technologies can help build confidence among providers and patients.\n",
      "\n",
      "This bibliography includes a variety of sources related to the use of artificial intelligence (AI) in healthcare. Key documents include an executive order by Joseph R. Biden Jr. on safe AI development and use [1], a proposal by the European Commission for harmonized rules on AI [2], and guidelines from the U.S. Food and Drug Administration on AI and machine learning in software [3,4]. There are also numerous academic articles on topics such as community-driven AI deployment in medical imaging [5], introducing AI in clinical practice [6], presenting machine learning model information to clinical end users [7], guidelines for clinical trial reports for AI interventions [8], reporting guideline for AI-centred diagnostic test accuracy studies [9], international consensus guideline for trustworthy AI in healthcare [10], minimum information about clinical AI modeling [11], developing reporting standards for AI in healthcare [12,14], and the future of digital health with federated learning [13]. Other sources discuss geographical variation and factors associated with non-small cell lung cancer [15], an open-source tool for quality control of MR imaging data [16], latent transformer models for out-of-distribution detection [17], predicting segmentation performance in the absence of ground truth [18], automatic correction of performance drift under acquisition shift in medical image classification [19], an open-source framework for deep learning in healthcare [20], degenerate feedback loops in recommender systems [21], creating value-based competition on results in healthcare [22], and use of AI-based tools for healthcare purposes from consumers' perspectives [23].\n"
     ]
    }
   ],
   "source": [
    "consolidated_summary = '\\n\\n'.join(summaries)\n",
    "print(consolidated_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
