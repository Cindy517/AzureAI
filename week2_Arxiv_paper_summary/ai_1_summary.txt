This is a list of contributors from various institutions who have equally contributed to a project or research. The work is related to the use of AI in radiology, focusing on safety and an end-to-end lifecycle approach. The contributors are M. Jorge Cardoso and Tiarna Lee from King's College London, Julia Moosbauer from Deepc GmbH and Ludwig Maximilian University Munich, Tessa S. Cook from the University of Pennsylvania, B. Selnur Erdal and Vikash Gupta from the Mayo Clinic, Brad Genereaux from Nvidia, and Bennett A. Landman from Vanderbilt University.

The incorporation of artificial intelligence (AI) into radiology presents opportunities for improved clinical care and efficiency. However, it also requires a careful approach to minimize potential risks. The focus should be on ensuring models meet the highest standards of safety, effectiveness, and efficacy. This includes rigorous pre-deployment evaluation and validation, implementation of input and output guardrails during production usage, and continuous post-deployment monitoring. AI models must adhere to the same safety, effectiveness, and efficacy standards as other established medical technologies. Trust in AI is earned by demonstrating adherence to these standards and instilling confidence among providers and patients. Collaborative efforts between healthcare systems, industry, academia, and government are essential to address the challenges involved. The safe and ethical deployment of AI systems in radiology requires comprehensive efforts throughout the entire AI product lifecycle. This includes pre-deployment due diligence, quality assurance testing, independent validation studies, live deployment monitoring, and post-deployment surveillance. An end-to-end approach to trustworthy AI deployment emphasizes safeguards across the AI lifecycle, including regulatory, clinical, technical, and ethical considerations. This approach fosters provider confidence in applying AI technologies to improve patient outcomes and quality of care.

The article discusses the pre-deployment phase of a radiology AI safety lifecycle approach. It highlights the importance of regulatory due diligence, ensuring AI models meet relevant regulations and standards such as FDA or EU's MDR guidelines for medical devices, including AI-based systems. It emphasizes the need for thorough due diligence by healthcare systems, hospitals, and AI marketplaces deploying these solutions to ensure safety, effectiveness, and regulatory compliance. The regulatory approval process varies in complexity between countries, but all aim to validate the device development methodology, algorithmic safety, and efficacy. AI deployment platforms also need to ensure models listed on them meet all regulatory requirements, including necessary certifications and clearances, compliance with registration obligations, and availability of comprehensive user-facing materials in relevant languages. The article also discusses the need for independent model validation and external retrospective data analysis or randomised controlled trials to verify model performance, calibration, value, and risk. A comprehensive validation is essential given the limited scale and diversity of clinical validation of AI models. The article also emphasizes the importance of model quality control, ensuring AI models perform well at every hospital they are deployed. This includes validating and documenting the model's accuracy and reliability using real-world datasets specific to each hospital and population. Quality control checks can be algorithmic or based on human experts, targeting model inputs, outputs, and population level statistics.

The software technology stack used to develop radiology AI models must meet high quality standards for safety and efficacy. When AI medical device manufacturers package these models, they need to ensure their software is free from errors, bugs, and security vulnerabilities. Quality assurance (QA) processes are implemented by the manufacturer, but models need continuous testing at every site they're deployed. Common issues that may occur during installation and deployment include unexpected algorithmic failures, incorrect inputs, and slow performance. To identify these, rigorous software QA testing is required before deployment and after integrating the software with the hospital's IT systems. Input data quality is crucial for ensuring AI models' safety and accuracy. Most models depend on system-level control to ensure data quality. Real-time quality control mechanisms should be in place to ensure that input data distribution doesn't drift from the one used during pre-deployment AI quality control. Output guardrails are also necessary to minimize AI risks at the patient level. Some AI manufacturers implement additional output guardrail mechanisms in the form of model confidence or uncertainty estimation, explicitly explainable models, or separate redundant quality control systems. However, a platform-level redundancy mechanism may be necessary to ensure safety across all deployed AI models. Both input and output data have to be closely monitored to ensure safe and accurate AI model performance. This includes checking data types, validating metadata, confirming protocol parameters, and assessing quality, among other measures.

The content discusses the importance of various safety mechanisms in AI medical devices. Operating thresholds should be defined based on model specifics and risk tolerance, with alerts triggered for any detected issues. Any low-quality algorithmic output should be deferred to a clinician and integrated with the patient's health record. This deferral mechanism should integrate seamlessly with clinical reporting systems and provide auditable logging of all deferrals. Model execution should be monitored to track key performance indicators such as runtime, failure and deferral rates, accuracy, precision, and recall. Specific metrics like model versioning, runtimes, hardware utilization, errors, audit logs, deferral rates, and accuracy metrics should be tracked for insight into model performance and to enable root cause analysis of problems. Finally, in the post-market surveillance phase, performance drift should be monitored to identify failures and problems that may only be visible at the population level. Changes in data acquisition, updated imaging scanners, and phenotype/population drift effects can cause algorithmic performance decay. If performance degradation is observed, a full clinical review and subsequent quality control study are necessary to prevent further impact on patient care.

AI model performance can drift post-deployment due to factors like data drift, phenotype drift, and concept drift. Monitoring metrics like accuracy, sensitivity, and specificity over time can detect performance degradation. If drift is suspected, a retrospective analysis should be undertaken, and the model might need retraining, adjustment of operating thresholds, or retirement. Model recalibration is also necessary as data and models drift, requiring adjustment of the operating point of a receiver-operating curve. Monitoring algorithmic sensitivity and specificity over time is crucial to ensure AI models behave appropriately. Bias and fairness are also significant, and model performance should be equitable across different population subgroups. If underperformance is found in certain population segments, a full clinical review should be conducted, followed by appropriate algorithmic remediation. An incident tracking system should be established to proactively collect and review data from devices placed on the market. This is vital for managing risks and ensuring the AI technology remains safe post-deployment. The value of AI models is also significant for building trust in the technology. Evaluating the impact of AI on metrics like clinical outcomes, patient satisfaction, operational efficiency, and costs is crucial. A holistic approach to trustworthy AI is needed, including rigorous pre-deployment evaluation, input and output guardrails, and continuous monitoring after deployment. Collaboration between stakeholders across healthcare systems, industry, academia, and government is vital to address the interdisciplinary challenges involved.

This bibliography includes a variety of sources related to the use of artificial intelligence (AI) in healthcare and related policies. 1. Executive order by Joseph R. Biden Jr. on the development and use of AI (2023). 2. Proposal by the European Commission for harmonised rules on AI (2021). 3. Two guidance documents from the FDA on 'Software as a Medical Device' and AI in software (2017 and 2023). 4. Various academic articles covering a range of topics, including radiological AI, presenting machine learning models to clinicians, reporting guidelines for AI-based clinical trials, consensus guidelines for trustworthy AI in healthcare, and the future of digital health with federated learning. 5. Articles discussing the need for local validation of AI models in health, geographical variations in lung cancer, quality control of MR imaging data, and automatic correction of performance drift in medical imaging. 6. Introduction of open-source tools for AI in healthcare such as 'MRQY' and 'MONAI'. 7. A book by Porter and Teisberg on creating value-based competition in healthcare (2006). 8. A survey study on the use of AI-based tools from consumers' perspectives (2020).